<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (1.8.0_144) on Fri Aug 07 15:05:07 EEST 2020 -->
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>NNNodeIntf</title>
<meta name="date" content="2020-08-07">
<link rel="stylesheet" type="text/css" href="../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="NNNodeIntf";
        }
    }
    catch(err) {
    }
//-->
var methods = {"i0":6,"i1":6,"i2":6,"i3":6,"i4":6,"i5":6,"i6":6,"i7":6,"i8":6,"i9":6,"i10":6,"i11":6,"i12":6,"i13":6,"i14":6,"i15":6,"i16":6,"i17":6,"i18":6,"i19":6,"i20":6,"i21":6};
var tabs = {65535:["t0","All Methods"],2:["t2","Instance Methods"],4:["t3","Abstract Methods"]};
var altColor = "altColor";
var rowColor = "rowColor";
var tableTab = "tableTab";
var activeTableTab = "activeTableTab";
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NNNodeIntf.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../index-files/index-1.html">Index</a></li>
<li><a href="../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../popt4jlib/neural/MultiClassSSE.html" title="class in popt4jlib.neural"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../popt4jlib/neural/OptFFNNRun.html" title="class in popt4jlib.neural"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../index.html?popt4jlib/neural/NNNodeIntf.html" target="_top">Frames</a></li>
<li><a href="NNNodeIntf.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">popt4jlib.neural</div>
<h2 title="Interface NNNodeIntf" class="title">Interface NNNodeIntf</h2>
</div>
<div class="contentContainer">
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>All Superinterfaces:</dt>
<dd>java.io.Serializable</dd>
</dl>
<dl>
<dt>All Known Subinterfaces:</dt>
<dd><a href="../../popt4jlib/neural/OutputNNNodeIntf.html" title="interface in popt4jlib.neural">OutputNNNodeIntf</a></dd>
</dl>
<dl>
<dt>All Known Implementing Classes:</dt>
<dd><a href="../../popt4jlib/neural/CategoricalXEntropyLoss.html" title="class in popt4jlib.neural">CategoricalXEntropyLoss</a>, <a href="../../popt4jlib/neural/CategoricalXEntropyLossW.html" title="class in popt4jlib.neural">CategoricalXEntropyLossW</a>, <a href="../../popt4jlib/neural/GELU.html" title="class in popt4jlib.neural">GELU</a>, <a href="../../popt4jlib/neural/HardThres.html" title="class in popt4jlib.neural">HardThres</a>, <a href="../../popt4jlib/neural/InputSignalMaxPosSelector.html" title="class in popt4jlib.neural">InputSignalMaxPosSelector</a>, <a href="../../popt4jlib/neural/Linear.html" title="class in popt4jlib.neural">Linear</a>, <a href="../../popt4jlib/neural/MultiClassSSE.html" title="class in popt4jlib.neural">MultiClassSSE</a>, <a href="../../popt4jlib/neural/Quadratic.html" title="class in popt4jlib.neural">Quadratic</a>, <a href="../../popt4jlib/neural/ReLU.html" title="class in popt4jlib.neural">ReLU</a>, <a href="../../popt4jlib/neural/Sigmoid.html" title="class in popt4jlib.neural">Sigmoid</a>, <a href="../../popt4jlib/neural/TanH.html" title="class in popt4jlib.neural">TanH</a>, <a href="../../popt4jlib/neural/TanH01.html" title="class in popt4jlib.neural">TanH01</a></dd>
</dl>
<hr>
<br>
<pre>public interface <span class="typeNameLabel">NNNodeIntf</span>
extends java.io.Serializable</pre>
<div class="block">common interface for a Neural Network node at any hidden layer.
 <p>Title: popt4jlib</p>
 <p>Description: A Parallel Meta-Heuristic Optimization Library in Java</p>
 <p>Copyright: Copyright (c) 2011-2020</p>
 <p>Company: </p></div>
<dl>
<dt><span class="simpleTagLabel">Version:</span></dt>
<dd>1.0</dd>
<dt><span class="simpleTagLabel">Author:</span></dt>
<dd>Ioannis T. Christou</dd>
</dl>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method.summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span id="t0" class="activeTableTab"><span>All Methods</span><span class="tabEnd">&nbsp;</span></span><span id="t2" class="tableTab"><span><a href="javascript:show(2);">Instance Methods</a></span><span class="tabEnd">&nbsp;</span></span><span id="t3" class="tableTab"><span><a href="javascript:show(4);">Abstract Methods</a></span><span class="tabEnd">&nbsp;</span></span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr id="i0" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#addPreviousWeightsRange-int-int-">addPreviousWeightsRange</a></span>(int&nbsp;start,
                       int&nbsp;end)</code>
<div class="block">adds the indices of weight variables that are input to nodes in previous
 layers that eventually connect to this one.</div>
</td>
</tr>
<tr id="i1" class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#eval-double:A-double:A-">eval</a></span>(double[]&nbsp;inputSignals,
    double[]&nbsp;inputWeights)</code>
<div class="block">method assumes that the input weights are available as a 1-D array of the 
 same dimension as the input signals to the node.</div>
</td>
</tr>
<tr id="i2" class="altColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#eval-double:A-double:A-int-">eval</a></span>(double[]&nbsp;inputSignals,
    double[]&nbsp;inputWeights,
    int&nbsp;inputWeightsStartOffset)</code>
<div class="block">method assumes that the weights are stored in a single, large 1-D array,
 and therefore, the third argument provides the index in the array at which
 the weights for this particular node are stored.</div>
</td>
</tr>
<tr id="i3" class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#evalB-double:A-double:A-">evalB</a></span>(double[]&nbsp;inputSignals,
     double[]&nbsp;inputWgts)</code>
<div class="block">works when biases are included.</div>
</td>
</tr>
<tr id="i4" class="altColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#evalB-double:A-double:A-int-">evalB</a></span>(double[]&nbsp;inputSignals,
     double[]&nbsp;inputWeights,
     int&nbsp;inputWeightsStartOffset)</code>
<div class="block">method assumes that connection weights as well as node biases are all 
 stored in a single, large 1-D array, and therefore, the third argument 
 provides the index in the array at which the weights for this particular 
 node (connection weights plus node bias immediately after them) are stored.</div>
</td>
</tr>
<tr id="i5" class="rowColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#evalPartialDerivativeB-double:A-int-double:A-double-">evalPartialDerivativeB</a></span>(double[]&nbsp;weights,
                      int&nbsp;index,
                      double[]&nbsp;input_signals,
                      double&nbsp;true_lbl)</code>
<div class="block">same as the <CODE>evalPartialDerivativeB(w,i,train_inst,train_lbl,p)</CODE>
 method except that the computations are for whole gradient calculations, in
 that the grad-vector cache is used without being reset in a full gradient 
 computation.</div>
</td>
</tr>
<tr id="i6" class="altColor">
<td class="colFirst"><code>double</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#evalPartialDerivativeB-double:A-int-double:A-double-java.util.HashMap-">evalPartialDerivativeB</a></span>(double[]&nbsp;weights,
                      int&nbsp;index,
                      double[]&nbsp;input_signals,
                      double&nbsp;true_lbl,
                      java.util.HashMap&nbsp;p)</code>
<div class="block">evaluates the partial derivative of this node (as a function of weights)
 with respect to the weight variable whose weight is given by the value of 
 the weights array in the given index.</div>
</td>
</tr>
<tr id="i7" class="rowColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getDirectInputWeightEndIndex--">getDirectInputWeightEndIndex</a></span>()</code>
<div class="block">gets the index of the last weight variable connected directly as  
 input to this node.</div>
</td>
</tr>
<tr id="i8" class="altColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getDirectInputWeightStartIndex--">getDirectInputWeightStartIndex</a></span>()</code>
<div class="block">gets the index of the first weight variable connected directly as  
 input to this node.</div>
</td>
</tr>
<tr id="i9" class="rowColor">
<td class="colFirst"><code><a href="../../popt4jlib/neural/FFNN4TrainB.html" title="class in popt4jlib.neural">FFNN4TrainB</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getFFNN4TrainB--">getFFNN4TrainB</a></span>()</code>
<div class="block">get the network this node belongs to.</div>
</td>
</tr>
<tr id="i10" class="altColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getNodeLayer--">getNodeLayer</a></span>()</code>
<div class="block">get the layer number of this node in the FFNN (</div>
</td>
</tr>
<tr id="i11" class="rowColor">
<td class="colFirst"><code>java.lang.String</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getNodeName--">getNodeName</a></span>()</code>
<div class="block">returns this node's name (usually just the type of the node, eg "ReLU").</div>
</td>
</tr>
<tr id="i12" class="altColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getPositionInLayer--">getPositionInLayer</a></span>()</code>
<div class="block">returns the position index of this node in the layer it's part of in the 
 FFNN.</div>
</td>
</tr>
<tr id="i13" class="rowColor">
<td class="colFirst"><code>int</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#getTotalNumWeights--">getTotalNumWeights</a></span>()</code>
<div class="block">gets the total number of weight variables (weights plus biases) for the 
 FFNN that this node participates in.</div>
</td>
</tr>
<tr id="i14" class="altColor">
<td class="colFirst"><code>boolean</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#isDropout--">isDropout</a></span>()</code>
<div class="block">return the dropout property of this node.</div>
</td>
</tr>
<tr id="i15" class="rowColor">
<td class="colFirst"><code>boolean</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#isWeightVariableAntecedent-int-">isWeightVariableAntecedent</a></span>(int&nbsp;index)</code>
<div class="block">returns true if and only if the index represented a connection weight that
 connects to a node that is eventually connected to this node.</div>
</td>
</tr>
<tr id="i16" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setDropout-boolean-">setDropout</a></span>(boolean&nbsp;val)</code>
<div class="block">after calling <CODE>setDropout(true)</CODE>, all <CODE>evalXXX()</CODE>
 of this node return true until <CODE>setDropout(false)</CODE> is called.</div>
</td>
</tr>
<tr id="i17" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setFFNN4TrainB-popt4jlib.neural.FFNN4TrainB-">setFFNN4TrainB</a></span>(<a href="../../popt4jlib/neural/FFNN4TrainB.html" title="class in popt4jlib.neural">FFNN4TrainB</a>&nbsp;ffnn)</code>
<div class="block">set the network this node belongs to.</div>
</td>
</tr>
<tr id="i18" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setNodeLayer-int-">setNodeLayer</a></span>(int&nbsp;layerno)</code>
<div class="block">sets the layer in the FFNN that this node belongs to.</div>
</td>
</tr>
<tr id="i19" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setPositionInLayer-int-">setPositionInLayer</a></span>(int&nbsp;pos_in_layer)</code>
<div class="block">sets the position of this node in the layer containing it.</div>
</td>
</tr>
<tr id="i20" class="altColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setTotalNumWeights-int-">setTotalNumWeights</a></span>(int&nbsp;num_weights)</code>
<div class="block">sets the total number of variables (weights plus biases) for the FFNN that
 this node participates in.</div>
</td>
</tr>
<tr id="i21" class="rowColor">
<td class="colFirst"><code>void</code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../popt4jlib/neural/NNNodeIntf.html#setWeightRange-int-int-">setWeightRange</a></span>(int&nbsp;start,
              int&nbsp;end)</code>
<div class="block">sets the range of indices in the weights vector variable that are fed into
 this node.</div>
</td>
</tr>
</table>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method.detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="eval-double:A-double:A-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>eval</h4>
<pre>double&nbsp;eval(double[]&nbsp;inputSignals,
            double[]&nbsp;inputWeights)</pre>
<div class="block">method assumes that the input weights are available as a 1-D array of the 
 same dimension as the input signals to the node.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>inputSignals</code> - double[]</dd>
<dd><code>inputWeights</code> - double[]</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="eval-double:A-double:A-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>eval</h4>
<pre>double&nbsp;eval(double[]&nbsp;inputSignals,
            double[]&nbsp;inputWeights,
            int&nbsp;inputWeightsStartOffset)</pre>
<div class="block">method assumes that the weights are stored in a single, large 1-D array,
 and therefore, the third argument provides the index in the array at which
 the weights for this particular node are stored.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>inputSignals</code> - double[]</dd>
<dd><code>inputWeights</code> - double[] assumed to contain all weights in a network</dd>
<dd><code>inputWeightsStartOffset</code> - int the index in the inputWeights that the
 weights for this node are stored.</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="evalB-double:A-double:A-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>evalB</h4>
<pre>double&nbsp;evalB(double[]&nbsp;inputSignals,
             double[]&nbsp;inputWeights,
             int&nbsp;inputWeightsStartOffset)</pre>
<div class="block">method assumes that connection weights as well as node biases are all 
 stored in a single, large 1-D array, and therefore, the third argument 
 provides the index in the array at which the weights for this particular 
 node (connection weights plus node bias immediately after them) are stored.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>inputSignals</code> - double[]</dd>
<dd><code>inputWeights</code> - double[] assumed to contain all weights in a network 
 including the biases for each node</dd>
<dd><code>inputWeightsStartOffset</code> - int the index in the inputWeights that the
 weights for this node are stored. Right after the weights the bias variable
 for the node follows</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="evalB-double:A-double:A-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>evalB</h4>
<pre>double&nbsp;evalB(double[]&nbsp;inputSignals,
             double[]&nbsp;inputWgts)</pre>
<div class="block">works when biases are included.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>inputSignals</code> - double[]</dd>
<dd><code>inputWgts</code> - double[]</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="getNodeName--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getNodeName</h4>
<pre>java.lang.String&nbsp;getNodeName()</pre>
<div class="block">returns this node's name (usually just the type of the node, eg "ReLU").</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>String</dd>
</dl>
</li>
</ul>
<a name="setFFNN4TrainB-popt4jlib.neural.FFNN4TrainB-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setFFNN4TrainB</h4>
<pre>void&nbsp;setFFNN4TrainB(<a href="../../popt4jlib/neural/FFNN4TrainB.html" title="class in popt4jlib.neural">FFNN4TrainB</a>&nbsp;ffnn)</pre>
<div class="block">set the network this node belongs to.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>ffnn</code> - FFNN4TrainB</dd>
</dl>
</li>
</ul>
<a name="getFFNN4TrainB--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getFFNN4TrainB</h4>
<pre><a href="../../popt4jlib/neural/FFNN4TrainB.html" title="class in popt4jlib.neural">FFNN4TrainB</a>&nbsp;getFFNN4TrainB()</pre>
<div class="block">get the network this node belongs to.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>FFNN4TrainB</dd>
</dl>
</li>
</ul>
<a name="setTotalNumWeights-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setTotalNumWeights</h4>
<pre>void&nbsp;setTotalNumWeights(int&nbsp;num_weights)</pre>
<div class="block">sets the total number of variables (weights plus biases) for the FFNN that
 this node participates in.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>num_weights</code> - int</dd>
</dl>
</li>
</ul>
<a name="getTotalNumWeights--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getTotalNumWeights</h4>
<pre>int&nbsp;getTotalNumWeights()</pre>
<div class="block">gets the total number of weight variables (weights plus biases) for the 
 FFNN that this node participates in.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>int</dd>
</dl>
</li>
</ul>
<a name="setWeightRange-int-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setWeightRange</h4>
<pre>void&nbsp;setWeightRange(int&nbsp;start,
                    int&nbsp;end)</pre>
<div class="block">sets the range of indices in the weights vector variable that are fed into
 this node. These are the weights that are directly input to this node and 
 includes the bias variable weight for this node.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>start</code> - int inclusive</dd>
<dd><code>end</code> - int inclusive</dd>
</dl>
</li>
</ul>
<a name="addPreviousWeightsRange-int-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>addPreviousWeightsRange</h4>
<pre>void&nbsp;addPreviousWeightsRange(int&nbsp;start,
                             int&nbsp;end)</pre>
<div class="block">adds the indices of weight variables that are input to nodes in previous
 layers that eventually connect to this one.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>start</code> - int inclusive</dd>
<dd><code>end</code> - int inclusive</dd>
</dl>
</li>
</ul>
<a name="getDirectInputWeightStartIndex--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getDirectInputWeightStartIndex</h4>
<pre>int&nbsp;getDirectInputWeightStartIndex()</pre>
<div class="block">gets the index of the first weight variable connected directly as  
 input to this node.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>int</dd>
</dl>
</li>
</ul>
<a name="getDirectInputWeightEndIndex--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getDirectInputWeightEndIndex</h4>
<pre>int&nbsp;getDirectInputWeightEndIndex()</pre>
<div class="block">gets the index of the last weight variable connected directly as  
 input to this node.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>int</dd>
</dl>
</li>
</ul>
<a name="isWeightVariableAntecedent-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>isWeightVariableAntecedent</h4>
<pre>boolean&nbsp;isWeightVariableAntecedent(int&nbsp;index)</pre>
<div class="block">returns true if and only if the index represented a connection weight that
 connects to a node that is eventually connected to this node.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>index</code> - int</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>boolean</dd>
</dl>
</li>
</ul>
<a name="setNodeLayer-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setNodeLayer</h4>
<pre>void&nbsp;setNodeLayer(int&nbsp;layerno)</pre>
<div class="block">sets the layer in the FFNN that this node belongs to.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerno</code> - int can be in {0,...#hidden_layers} with #hidden_layers
 indicating the output layer.</dd>
</dl>
</li>
</ul>
<a name="getNodeLayer--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getNodeLayer</h4>
<pre>int&nbsp;getNodeLayer()</pre>
<div class="block">get the layer number of this node in the FFNN (</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
</dl>
</li>
</ul>
<a name="setPositionInLayer-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setPositionInLayer</h4>
<pre>void&nbsp;setPositionInLayer(int&nbsp;pos_in_layer)</pre>
<div class="block">sets the position of this node in the layer containing it.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>pos_in_layer</code> - int in {0,...,#nodes_in_layer-1}</dd>
</dl>
</li>
</ul>
<a name="getPositionInLayer--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>getPositionInLayer</h4>
<pre>int&nbsp;getPositionInLayer()</pre>
<div class="block">returns the position index of this node in the layer it's part of in the 
 FFNN.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>int in {0,...,#nodes_in_layer-1}</dd>
</dl>
</li>
</ul>
<a name="evalPartialDerivativeB-double:A-int-double:A-double-java.util.HashMap-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>evalPartialDerivativeB</h4>
<pre>double&nbsp;evalPartialDerivativeB(double[]&nbsp;weights,
                              int&nbsp;index,
                              double[]&nbsp;input_signals,
                              double&nbsp;true_lbl,
                              java.util.HashMap&nbsp;p)</pre>
<div class="block">evaluates the partial derivative of this node (as a function of weights)
 with respect to the weight variable whose weight is given by the value of 
 the weights array in the given index.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>weights</code> - double[] all variables (including biases) array</dd>
<dd><code>index</code> - int the index of the partial derivative to take</dd>
<dd><code>input_signals</code> - double[] the input signals for the network (an 
 instance of the training data)</dd>
<dd><code>true_lbl</code> - double the true label corresponding to the input_signals
 vector</dd>
<dd><code>p</code> - HashMap includes the train-data matrix and train-labels array.</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="evalPartialDerivativeB-double:A-int-double:A-double-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>evalPartialDerivativeB</h4>
<pre>double&nbsp;evalPartialDerivativeB(double[]&nbsp;weights,
                              int&nbsp;index,
                              double[]&nbsp;input_signals,
                              double&nbsp;true_lbl)</pre>
<div class="block">same as the <CODE>evalPartialDerivativeB(w,i,train_inst,train_lbl,p)</CODE>
 method except that the computations are for whole gradient calculations, in
 that the grad-vector cache is used without being reset in a full gradient 
 computation.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>weights</code> - double[] all variables (including biases) array</dd>
<dd><code>index</code> - int the index of the partial derivative to take</dd>
<dd><code>input_signals</code> - double[] the input signals for the network (an 
 instance of the training data)</dd>
<dd><code>true_lbl</code> - double the true label corresponding to the input_signals
 vector</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>double</dd>
</dl>
</li>
</ul>
<a name="setDropout-boolean-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setDropout</h4>
<pre>void&nbsp;setDropout(boolean&nbsp;val)</pre>
<div class="block">after calling <CODE>setDropout(true)</CODE>, all <CODE>evalXXX()</CODE>
 of this node return true until <CODE>setDropout(false)</CODE> is called.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>val</code> - boolean</dd>
</dl>
</li>
</ul>
<a name="isDropout--">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>isDropout</h4>
<pre>boolean&nbsp;isDropout()</pre>
<div class="block">return the dropout property of this node.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>boolean</dd>
</dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="class-use/NNNodeIntf.html">Use</a></li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../index-files/index-1.html">Index</a></li>
<li><a href="../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../popt4jlib/neural/MultiClassSSE.html" title="class in popt4jlib.neural"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../popt4jlib/neural/OptFFNNRun.html" title="class in popt4jlib.neural"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../index.html?popt4jlib/neural/NNNodeIntf.html" target="_top">Frames</a></li>
<li><a href="NNNodeIntf.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li>Constr&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
