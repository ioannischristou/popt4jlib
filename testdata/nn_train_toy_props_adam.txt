dbglvl,1
# the following defines an ANN and weights on the nn's connections
# the ANN looks like this:
# INP L0    OUT
# x1  N00
#           N10
# x2  N01   
class,outputlayer,popt4jlib.neural.Linear
arrayofcopies,layer1_arr,2,popt4jlib.neural.Linear
array,hiddenlayers,[Ljava.lang.Object;,layer1_arr
# normalized train data:
matrix,ffnn.traindata,testdata/toy_train.dat
# train labels
dblarray,ffnn.trainlabels,testdata/toy_train.dat.lbls
class,costfunc,popt4jlib.neural.costfunction.MSSE
# optimizer props
class,ffnn.mainoptimizer,popt4jlib.neural.Adam4FFNN
class,adam.function,popt4jlib.neural.FFNN4TrainB,hiddenlayers,outputlayer,costfunc
ref,opt.function,adam.function
opt.numthreads,8
# line below causes non-deterministic function evaluations, of small batches of
# input vectors, but runs a lot faster!
#ffnn.randombatchsize,10000
rndgen,7,4
class,adam.gradient,popt4jlib.neural.FFNN4TrainBGrad,opt.function,2
#adam.a,0.5
adam.a,0.9
adam.gtol,0.0
adam.maxiters,4
adam.numdimensions,9
array,adam.init_weights,double,1,1, 0, 1,1, 0, 1, 1, 1
# output file below used by [DGA,AVD]FFNNTest class
ffnn.outputlabelsfile,testdata/result_toy_trainlabels.dat
