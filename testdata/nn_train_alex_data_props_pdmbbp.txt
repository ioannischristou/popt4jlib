dbglvl,1
rndgen,7
# the following defines an ANN and weights on the nn's connections
# the ANN looks like this:
# INP L0    L1    OUT
# x1
# x2  N00   N10   
# x3  N01   N11   
# x4  ...   ...   y
# x5  ...   ...
# ... ...   N20
# ... N09
# x36
class,outputlayer,popt4jlib.neural.Sigmoid,1.0
arrayofcopies,layer1_arr,40,popt4jlib.neural.Linear
arrayofcopies,layer2_arr,10,popt4jlib.neural.Sigmoid,1.0
arrayofcopies,layer3_arr,10,popt4jlib.neural.Sigmoid,1.0
arrayofcopies,layer4_arr,5,popt4jlib.neural.ReLU
array,hiddenlayers,[Ljava.lang.Object;,layer1_arr,layer2_arr,layer3_arr,layer4_arr
# normalized train data:
matrixN01,ffnn.traindata,testdata/traindata1.dat
# train labels
dblarray,ffnn.trainlabels,testdata/trainlabels1.dat
class,costfunc,popt4jlib.neural.costfunction.MSSE
# optimizer props
class,ffnn.mainoptimizer,popt4jlib.neural.PDMiniBatchBackPropagation
class,opt.function,popt4jlib.neural.FFNN4TrainB,hiddenlayers,outputlayer,costfunc,4
pdmbbp.num_threads,4
array,pdmbbp.learning_rates,double,0.3
pdmbbp.normgrad,true
pdmbbp.c1,500.0
pdmbbp.c2,500.0
ffnn.minibatchsize,500
pdmbbp.num_epochs,200
pdmbbp.momentum,0.2
#sgd.numdimensions,XXX
# output file below used by [DGA,AVD]FFNNTest class
ffnn.outputweightsfile,testdata/result_trainweights1.dat